```
title:             딥러닝을 위한 정말정말 기초 지식1
date:              2019-07-18 01:12:00
author:            ho95
githublink:        https://github.com/Ho95
tags:              [cuda,machinelearning]
```



# 딥러닝을 위한 짜투리 잡지식

이번 여름 IDEC 강좌중 충북대에서 진행되는 "신경망(딥러닝)의 고성능 구현" 강좌에 관심이

생겨 수강하려고 보니 선수 과목으로 CUDA 프로그래밍이 있었습니다.

학부에선 듣도보도 못했으므로... 기초적인 부분을 직접 검색하여 정리하였습니다. 



## CUDA

CUDA(쿠다)라는 단어는 보통 엔비디아의 지포스 그래픽을 구매하실때 쉽게 찾을 수 있습니다

[^CUDA는 NVIDIA가 만든 병렬 컴퓨팅 플랫폼 및 API 모델이다. CUDA 플랫폼은 GPU 의 가상 명령어셋을 사용할 수 있도록 만들어주는 소프트웨어 레이어이며, NVIDIA가 만든 CUDA 코어가 장착된 GPU에서 작동한다.]: 

나무위키에서 정의된 쿠다는 위와 같습니다.  CPU의 ALU같은 느낌으로...

그래픽카드의 연산을 위한 라이브러리라 생각하면 될 것 같습니다. (라데온은 스트림 프로세서, OpenCL)

SW이기때문에 구현된 언어가 있습니다. 초기에는 C,C++이였으나, 최근에는 다른 언어도 지원한다고 하네요.( 안되는 것 있을 수 있음) 자세한 것은 https://namu.wiki/w/CUDA 참고.



## CPU와 GPU의 차이

CISC 기반의 CPU는 다양한 명령어를 지원하고 그로인해 내부 구조가 복잡하게 설계되어있습니다.(RISC CPU는 다를수도..)

하지만 GPU는 특화된 연산만을 처리하기 위해 불필요한 부분을 제거하고 특화 처리만 남겨놔서 비교적 간단한 구조입니다. 하지만 특화되어 있기때문에 단독으로 작업을 수행할 수 없으며 CPU의 제어를 따릅니다.



## GPGPU(General Purpose computing on Grphics Processing Unit)

GPGPU는 GPU를 이용해서 CPU가 맡았던 연산에 사용함으로 CPU 단독으로 쓰일때보다

연산속도를 향상시킵니다. 머신러닝에서 GPGPU를 사용합니다. (GPU가 병렬처리를 잘하기 때문...)

하지만 대용량 연산 같이 병렬처리시 이득이 되는 경우가 아니라면 오히려 CPU 단독으로 사용시가 더

좋을 수 있습니다. 적절히 섞어 사용하는 것이 중요합니다.

그리고 CPU와 GPU간의 데이터 교환에서는 PCI-E의 대역폭이 중요합니다.

컴퓨터 구조 과목에서도 CPU와 주메모리 사이의 속도차이가 커서 지연이 있는 것처럼

GPU는 CPU와 PCI-E를 통해 통신합니다. 근대 대역폭이 충분하지 않으면 아무리 연산을 빠르게 끝내도

데이터를 넘기는데, 시간이 걸립니다. 지연이 생깁니다.

그래서 여기서 나온 아이디어가 APU라고 하네요. (CPU와 GPU가 같이 있는 구조)

자세한 내용은 https://namu.wiki/w/GPGPU 참고